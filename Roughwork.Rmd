---
title: "Practical Project 1"
author: "M Meyer (22675760)"
date: "2024-10-01"
output:
  bookdown::gitbook:
    number_sections: FALSE
    toc_depth: 4  # Adjust depth if needed
    config:
      toc:
        scroll_highlight: true
        before: null
        after: null
      sidebar: true  # Adds a sidebar for the table of contents
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

poldat <- read.csv("Political spectrum.csv", header=TRUE)
poldat <- poldat[,-1]
# Save the numeric version before making some variables factors.
polnum <- poldat

# Make sure to specify ordinal variables
poldat$LGBT <- factor(poldat$LGBT, 
                             levels = 1:6, 
                             ordered = TRUE)  # Ensure it's ordered
poldat$mLeft <- factor(poldat$mLeft, 
                             levels = 1:8, 
                             ordered = TRUE)  # Ensure it's ordered
poldat$mRight <- factor(poldat$mRight, 
                             levels = 1:8, 
                             ordered = TRUE)  # Ensure it's ordered
head(poldat)

# install.packages("~/Desktop/Bootstrap B/Practical Project/robustbase_0.99-4-1.tgz", repos = NULL, type = .Platform$pkgType)

library(VIM) # struggle!
library(mice)
library(ggplot2)
library(bipl5)
library(factoextra) # For PCA visualization
library(gridExtra)
library(randomForest)
library(plotly)
library(mgcv)
library(MASS)
library(pvclust)
library(boot)
library(fpc)

# install.packages("bookdown")
```

*Remember to set the specific variables as ordinal variables!! Look at data description.*

## Expected Analyses {.unnumbered}

Each of the 4 sections should contain at least one application of resampling. Provide the details in your technical report; your report to your friend only needs to show as much as you need to demonstrate to him whether the approach "worked" or not.

Before starting with the analyses, I just check whether there are any duplicated data entries. This might be important later on. We see that there are not any duplicate cases.

```{r}
sum(duplicated(poldat))
```

### 1. Handle the missing values in the dataset in a way that you feel is justified in the circumstances (include details in technical report; use resampling)

```{r}
aggr(poldat, prop = c(TRUE, FALSE))$missings
```

Economic and environment always missing together `pCongress`, `pBusiness`, `tCongress`, `tBusiness` also missing together (majority of missing combos is of all 4 of these)

A `matrixplot` visualizes all cells of the data matrix by rectangles. Observed data are shown in a continuous grey-black color scheme (the darker the color, the higher the value), while missing values are highlighted in red. It's a good practice to sort the data by one of the incomplete variables - it makes the plot easier to interpret.

In the graph you can see how strong the relationship in missing values is between the four business and congress variables and how their missing values are almost clustered.

Safe to say the data is not MCAR.

```{r}
matrixplot(poldat, sortby = c('tCongress'))
```

Now using the `mice` package. Columns that need not be imputed have the empty method `""` and `pmm` stands for Predictive mean matching.

```{r warning=FALSE}
set.seed(777)
md.pattern(poldat, plot=TRUE)

# Impute the missing values
polImp <- mice(poldat ,m=5,maxit=10,meth='pmm',seed=500, print=FALSE)
polnumImp <- mice(polnum ,m=5,maxit=10,meth='pmm',seed=500, print=FALSE)
# summary(polImp)


poldatImp <- complete(polImp, 1)
polNumImp <- complete(polnumImp,1)
head(poldatImp)
```

Predictive Mean Matching (PMM) in the mice package involves a form of **resampling**. PMM is a semi-parametric imputation method that combines the benefits of predictive modeling and resampling techniques to impute missing values in a realistic way, especially for continuous variables.

*I want to fit a kernel and see if the form is super different after imputing the missing values, but I don't know how to do that for data with more than one predictor variable... The example below just gives code from the `stats` package manual on how the `kernel` and `density` stuff works.*

Because even when I ask ChatGPT then it uses a visual via `ggplot` and `geom_density()` to plot the response and only one predictor variable... So I have no idea how to test whether my data still looks good...

```{r}
## The available kernels:
(kernels <- eval(formals(density.default)$kernel))


## show the kernels in the R parametrization
plot (density(0, bw = 1), xlab = "",
      main = "R's density() kernels with bw = 1")
for(i in 2:length(kernels))
   lines(density(0, bw = 1, kernel =  kernels[i]), col = i)
legend(1.5,.4, legend = kernels, col = seq(kernels),
       lty = 1, cex = .8, y.intersp = 1)
```

### 2. Conduct a principal components analysis using all the variables in the dataset:

#### Investigate the proportion of variation explained by the first or the first two components (some definitions of the political spectrum use economic and social axes separately)

Also... **we have to implement resampling somewhere...**. So perhaps use bootstrap to get the loadings of the first two PCs? Look at slide 14 from Assignment 13b.

We can only do scaling on numeric variables... so we use the dataset that still only has numeric variables here.

Or do we just ditch the scaling?

```{r}
# Scale the data NB for PCA. Note here we are using the numeric dataset
pol_scaled <- data.frame(scale(polNumImp))

# Perform PCA
pca_result <- prcomp(pol_scaled)

# Step 4: Check the summary of the PCA results
summary(pca_result)

# Step 5: Visualize the PCA (optional)
# Plot variance explained by each principal component
fviz_eig(pca_result)

# Plot individuals (samples) on the principal component map
fviz_pca_ind(pca_result, 
             addEllipses = TRUE # Add confidence ellipses
             ) + 
  theme_minimal()

# Plot variables (features) on the principal component map
fviz_pca_var(pca_result, 
             col.var = "contrib", # Color by contributions to the PCs
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))

```

When two vectors are close, forming a small angle, the two variables they represent are positively correlated.

**REPEAT BUT LEAVE OUT `mRight` AND `mLeft`**. Also note that I'm not sure how to interpret the individual plot.

It seems that leaving them out gives super non-informative answers and plots so continue using `mLeft` and `mRight` and just mention that this was considered.

```{r}
# Scale the data NB for PCA. Note here we are using the numeric dataset
pol_scaled0 <- scale(polNumImp[,1:12])

# Perform PCA
pca_result0 <- prcomp(pol_scaled0, center = TRUE, scale. = TRUE)

# Step 4: Check the summary of the PCA results
summary(pca_result0)

# Step 5: Visualize the PCA (optional)
# Plot variance explained by each principal component
fviz_eig(pca_result0)

# Plot individuals (samples) on the principal component map
fviz_pca_ind(pca_result0, 
             addEllipses = TRUE # Add confidence ellipses
             ) + 
  theme_minimal()

# Plot variables (features) on the principal component map
fviz_pca_var(pca_result0, 
             col.var = "contrib", # Color by contributions to the PCs
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))

```

So together, the first two principal components explain 25% of the variation in the data.

More plots (that I have no idea how to interpret):

```{r}
pca_ind <- get_pca_ind(pca_result)

# Create a data frame with the first principal component and the species
pca_comps <- data.frame(PC1 = pca_ind$coord[, 1], 
                        PC2 = pca_ind$coord[, 2],
                        LeftScale = as.factor(pol_scaled$mLeft),
                        RightScale = as.factor(pol_scaled$mRight)
                        )

# Create boxplot of the first principal component
boxplotL1 <- ggplot(pca_comps, aes(x = LeftScale, y = PC1)) +
              geom_boxplot(fill = "lightblue", color = "darkblue") +
              labs(title = "Boxplot of PC1 - mLeft",
                   x = "mLeft", 
                   y = "PC1") +
  scale_x_discrete(labels = as.character(1:8)) +
              theme_minimal()

# Create boxplot of the first principal component
boxplotR1 <- ggplot(pca_comps, aes(x = RightScale, y = PC1)) +
              geom_boxplot(fill = "lightgreen", color = "darkgreen") +
              labs(title = "Boxplot of PC1 - mRight",
                   x = "mRight", 
                   y = "PC1") +
  scale_x_discrete(labels = as.character(1:8)) +
              theme_minimal()

# Create boxplot of the first principal component
boxplotL2 <- ggplot(pca_comps, aes(x = LeftScale, y = PC2)) +
              geom_boxplot(fill = "lightblue", color = "darkblue") +
              labs(title = "Boxplot of PC2 - mLeft",
                   x = "mLeft", 
                   y = "PC2") +
  scale_x_discrete(labels = as.character(1:8)) +
              theme_minimal()

# Create boxplot of the first principal component
boxplotR2 <- ggplot(pca_comps, aes(x = RightScale, y = PC2)) +
              geom_boxplot(fill = "lightgreen", color = "darkgreen") +
              labs(title = "Boxplot of PC2 - mRight",
                   x = "mRight", 
                   y = "PC2") +
  scale_x_discrete(labels = as.character(1:8)) +
              theme_minimal()

grid.arrange(boxplotL1, boxplotR1, boxplotL2, boxplotR2, ncol = 2, nrow = 2)
```

#### Interpret the coefficients and/or the correlations between the variables and the components and compare your interpretation to how the left-wing/right-wing political spectrum is viewed in the country where the data were collected.

See note for next section.

#### Could the first or first two components serve as a scale for left/right political orientation?

Answer this question by looking at whether the way they are correlated relates to what we see as left-wing and right-wing. And note where it does not.

### 3. Consider whether the principal components from step 2 could be used to "smooth out" the results of the two variables recording the statements the respondents agreed with, mLeft and mRight:

#### Plot the first principal component against each of mLeft and mRight, and add bootstrapped loess curves to the plots;

I will be continuing with the scaled dataset for the sake of consistency.

For more information on *loess* smoothing look at Slide 20 of Assignment 15.

Now adding a *loess* curve using bootstrap. First do it for `mLeft` as illustration. Still unsure of what value to use for the `span` parameter.

We noted earlier that our dataset doesn't have any duplicate cases so we resample cases and not residuals.

Not sure why the x-axis labels aren't showing but at least the plot is still easily interpretable.

```{r}
# Create a data frame with the first principal component and the response variable (Species)
pc1_data <- data.frame(PC1 = pca_ind$coord[, 1], 
                     mLeft = pol_scaled$mLeft,
                     mRight = pol_scaled$mRight)  # Replace with your response variables

pc12_data <- data.frame(PC1 = pca_ind$coord[, 1], 
                        PC2 = pca_ind$coord[, 2],
                     mLeft = pol_scaled$mLeft,
                     mRight = pol_scaled$mRight)  # Replace with your response variables
# Set the seed
set.seed(777)

# Number of bootstrap samples
B <- 200

# Grid of x values where the LOESS curve will be estimated
x_seq <- sort(unique(pc1_data$mLeft))

# Matrix to store LOESS predictions for each bootstrap sample
loess_preds <- matrix(NA, nrow = length(x_seq), ncol = B)

# Perform bootstrap sampling and fit LOESS models
for (b in 1:B) {
  # Generate a bootstrap sample from the data
  boot_sample <- data.frame(pc1_data[sample(1:nrow(pc1_data), replace = TRUE), ])
  
  # Fit LOESS model to the bootstrap sample
  loess_fit <- loess(PC1 ~ mLeft, data = boot_sample, span = 0.75)
  
  # Predict y values for the grid of x values
  loess_preds[, b] <- predict(loess_fit, newdata = data.frame(mLeft = x_seq))
}

# Adaptive estimation: Average the predictions across bootstrap samples
loess_adaptive_estimate <- rowMeans(loess_preds, na.rm = TRUE)
loess_curve <- data.frame(x = x_seq, y = loess_adaptive_estimate)

# Create a data frame for all the bootstrap LOESS curves
loess_bootstrap_df <- data.frame(
  x = rep(x_seq, B),
  y = as.vector(loess_preds),
  group = rep(1:B, each = length(x_seq))
)

# Plot original data, all bootstrap LOESS curves, and adaptive LOESS estimate
ggplot(pc1_data, aes(x = mLeft, y = PC1)) +
  geom_point(aes(color = mLeft), alpha = 0.5) +  # Scatter plot
  scale_color_gradient(low = "turquoise", high = "salmon") +  # Optional color gradient
  geom_line(data = loess_bootstrap_df, aes(x = x, y = y, group = group),
            color = "lightblue", linewidth = 0.5, alpha = 0.2) +  # Bootstrap LOESS curves
  geom_line(data = loess_curve, aes(x = x, y = y), color = "dodgerblue4", size = 1) +  # Adaptive LOESS curve
  labs(title = "Bootstrapped LOESS Curve",
       x = "mLeft", y = "PC1") + 
  scale_x_discrete(labels = as.character(1:8))


#### NOW LOESS CURVE FITTED TO MRIGHT ###

x_seq <- sort(unique(pc1_data$mRight))

# Matrix to store LOESS predictions for each bootstrap sample
loess_preds <- matrix(NA, nrow = length(x_seq), ncol = B)

# Perform bootstrap sampling and fit LOESS models
for (b in 1:B) {
  # Generate a bootstrap sample from the data
  boot_sample <- data.frame(pc1_data[sample(1:nrow(pc1_data), replace = TRUE), ])
  
  # Fit LOESS model to the bootstrap sample
  loess_fit <- loess(PC1 ~ mRight, data = boot_sample, span = 0.75)
  
  # Predict y values for the grid of x values
  loess_preds[, b] <- predict(loess_fit, newdata = data.frame(mRight = x_seq))
}

# Adaptive estimation: Average the predictions across bootstrap samples
loess_adaptive_estimate <- rowMeans(loess_preds, na.rm = TRUE)
loess_curve <- data.frame(x = x_seq, y = loess_adaptive_estimate)

# Create a data frame for all the bootstrap LOESS curves
loess_bootstrap_df <- data.frame(
  x = rep(x_seq, B),
  y = as.vector(loess_preds),
  group = rep(1:B, each = length(x_seq))
)

# Plot original data, all bootstrap LOESS curves, and adaptive LOESS estimate
ggplot(pc1_data, aes(x = mRight, y = PC1)) +
  geom_point(aes(color = mRight), alpha = 0.5) +  # Scatter plot
  scale_color_gradient(low = "turquoise", high = "salmon") +  # Optional color gradient
  geom_line(data = loess_bootstrap_df, aes(x = x, y = y, group = group),
            color = "pink", linewidth = 0.5, alpha = 0.2) +  # Bootstrap LOESS curves
  geom_line(data = loess_curve, aes(x = x, y = y), color = "magenta4", size = 1) +  # Adaptive LOESS curve
  labs(title = "Bootstrapped LOESS Curve",
       x = "mRight", y = "PC1") +
  scale_x_discrete(labels = as.character(1:8))

```

From the *loess* curves it seems that the majority of the uncertainty lies around point 6 on the `mLeft` and `mRight` scale. This should be investigated.

#### Decide on a suitable model and fit a model predicting PC1 from mLeft and mRight.

From the plots above, the curves look very linear.

The covariates are random and not fixed (even though the scale is fixed), so we use case resampling and not residual resampling. But when I fit my model, I need to remember to look at the *heteroschedasticity* of the model!

Test out a linear model.

First look at the normality of the response variable. From the histogram it looks super normally distributed so we don't have to use GLMs, we can just do normal linear model

```{r}
ggplot(pc1_data, aes(x = PC1)) +
  geom_histogram(binwidth = 0.75, fill = "blue", color = "black", alpha = 0.7) +
  labs(title = "Histogram of PC1 response",
       x = "PC1",
       y = "Count") +
  theme_minimal()
```

Now fit the linear model and from the residual plot we can see that there is no heteroschedasticity. This means that we don't need to use wild bootstrap in our resampling procedure.

From the model output, the $R^2$ isn't amazing, but combined with the low p-value it shows the model is a good fit. However the low $R^2$ might be related to the fact that the discreteness of the estimated responses leads to a loss of information and limits the response to a set of values that is much smaller than the original response.

```{r}
linmod0 <- lm(PC1 ~ 1+ mLeft + mRight + mLeft*mRight, data = pc1_data)
summary(linmod0)

# remove the interaction and intercept
linmod1 <- lm(PC1 ~ mLeft + mRight - 1, data = pc1_data)
summary(linmod1)
AIC(linmod1)

# Extract residuals
modres <- linmod1$residuals

# Create a data frame for plotting
residuals_data <- data.frame(modres)

# Plot the residuals
ggplot(residuals_data, aes(x = 1:nrow(residuals_data), y = modres)) +
  geom_point(size = 0.75) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  labs(title = "Residuals Plot",
       x = "Index",
       y = "Residuals") +
  theme_minimal()

ggplot(residuals_data, aes(x = modres)) +
  geom_histogram(bins=15, fill = "lightblue3", color = "black", alpha = 0.7) +
  labs(title = "Histogram of PC1 response",
       x = "Residuals",
       y = "Count") +
  theme_minimal()
```

Use bootstrap to investigate the standard errors of the model parameters. Note that when you do bootstrap on this, you assume the model form is correct.

Note that the estimate for $\sigma^2$ relies on the model being correct. So if our assumption of a linear relationship was wrong, then our $\widehat \sigma^2$ would have been inflated. There is a good correspondence between bootstrap standard error and that from the model output, which is an indicator of a good model fit.

The bias is also quite small, which is good.

```{r}
boot.fn  = function(data, index) return(coef(lm(PC1 ~ mLeft + mRight - 1, data = data, subset = index)))

boot(pc1_data, boot.fn, 500)
```

3D Plot:

```{r warning=FALSE}
# Create a grid of values for mRight and mLeft
mRight_grid <- sort(unique(pc1_data$mRight))
mLeft_grid <- sort(unique(pc1_data$mLeft))

# Create a grid for prediction
grid <- expand.grid(mLeft = mLeft_grid, mRight = mRight_grid)

# Predict PC1 on the grid
grid$PC1 <- predict(linmod1, newdata = grid)

# Convert the predicted values into a matrix to use for plotting the surface
PC1_matrix <- matrix(grid$PC1, nrow = length(mRight_grid), ncol = length(mLeft_grid))

# Plot the original data points in 3D and overlay the linear model plane
plot_ly() %>%
  add_markers(data = pc1_data, x = ~mRight, y = ~mLeft, z = ~PC1, color = ~PC1,
              colors = "Blues", marker = list(size = 2)) %>%
  add_surface(x = ~mRight_grid, y = ~mLeft_grid, z = ~PC1_matrix, 
              showscale = FALSE, opacity = 0.5) %>%
  layout(scene = list(xaxis = list(title = 'mRight'),
                      yaxis = list(title = 'mLeft'),
                      zaxis = list(title = 'PC1')),
         title = "3D Plot of Data with Linear Model Overlay")
```

Next, we consider a Generalized Additive Model (GAM) to address the issue of the discreteness of the response.GAMs extend linear models by allowing the relationship between the predictors and the response to be non-linear. They can model more flexible relationships while maintaining the continuity of the response variable.

The default value of `family=gaussian()` is used since our response variable is continuous and normally distributed (as seen in the histogram from earlier). For the Gaussian family, the method used to fit the model is Generalized Cross-Validation and uses Mallow's $C_p$ criterion to penalize the model for complexity. This adjusted version of GCV includes a bias-correction term that improves model selection when fitting a Generalized Additive Model (GAM). We use the `identity` link function, since the response variable is continuous and unbounded.

We set `k=8` to reduce the number of basis functions to match the number of unique values, which means it tries to estimate up to 7 degrees of freedom.

The AIC for the GAM is slightly better (lower) than the linear model, so we choose this as our model. The intercept term is removed since it is not statistically significant.

*Although I feel like I'm still not 100% sure of how this function works. So look into the theory behind it so that I can defend my work.*

```{r}
gam_model0 <- gam(PC1 ~ s(mLeft, k=8) + s(mRight, k=8), data = pc1_data)
summary(gam_model0)
AIC(gam_model0)

gam_model1 <- gam(PC1 ~ s(mLeft, k=8) + s(mRight, k=8) -1, data = pc1_data)
summary(gam_model1)
AIC(gam_model1)

boot.gam  = function(data, index) return(coef(gam(PC1 ~ s(mLeft, k=8) + s(mRight, k=8) -1, data = data, subset = index)))

boot(pc1_data, boot.gam, 500)
```

3D Plot:

```{r}
# Create a grid for prediction
grid2 <- expand.grid(mLeft = mLeft_grid, mRight = mRight_grid)

# Predict PC1 on the grid
grid2$PC1 <- predict(gam_model1, newdata = grid2)

# Convert the predicted values into a matrix to use for plotting the surface
PC1_matrix2 <- matrix(grid2$PC1, nrow = length(mRight_grid), ncol = length(mLeft_grid))

# Plot the original data points in 3D and overlay the linear model plane
plot_ly() %>%
  add_markers(data = pc1_data, x = ~mRight, y = ~mLeft, z = ~PC1, color = ~PC1,
              colors = "Blues", marker = list(size = 2)) %>%
  add_surface(x = ~mRight_grid, y = ~mLeft_grid, z = ~PC1_matrix2, 
              showscale = FALSE, opacity = 0.5) %>%
  layout(scene = list(xaxis = list(title = 'mRight'),
                      yaxis = list(title = 'mLeft'),
                      zaxis = list(title = 'PC1')),
         title = "3D Plot of Data with Linear Model Overlay")
```

#### Could the predicted value $\hat{y}$ from this model function as a smoothed composite of mLeft and mRight? Why or why not?

I think it depends on what you see as smooth, because using a linear model means with predictor variables that are discrete, means that our response can only take on $8^2=64$ unique values. As mentioned earlier, this discreteness could lead to a loss of information, since the model isn't able to cover the whole range of the possible response values.

The GAM's predicted value can be seen as a smoothed composite as it maintains the continuity of `PC1` while taking the discrete inputs.

### 4. Investigate whether a cluster-based solution would be a better option:

#### Form clusters using the method you think would be most suited to the problem, and evaluate the quality of the clustering. As these are convenience clusters, you may prefer to use the more sharply separated cluster allocations from e.g. a discriminant analysis.

**For more information on clustering and bootstrap, look at Assignment 13a.**

Look into the package `pvclust`, it is referred to in an article that we discuss in our class Assignment.

Also note the notion of **Multiscale** bootstrap. The package `fcp` is a comprehensive and flexible package in this context. Note that `fcp::clusterboot()` calculates cluster stability indices using various methods, including bootstrap.

---

I am unsure of whether we should use all the variables in the clustering procedure or only `mRight` and `mLeft`....

Using `nselectboot` on the original dataset, we find that it just takes the maximum number of clusters that it can, so that is not helpful.

We also try it on the data with the first two principal components, as well as `mRight` and `mLeft`. But now we sit with the problem of still having way too many clusters...

**We will be using hierarchical clustering as the clusters are not necessarily well-separated and and also not necessarily spherical.**

Use **average linkage** as it provides a balance between the extremes of single linkage (which can form long chain-like clusters) and complete linkage (which can form very tight clusters). This method doesn’t force clusters to be as well-separated as complete linkage. It allows for some shading between clusters, which is fine since we don't mind the shading in this case. Unlike single linkage, which might create many small, poorly-separated clusters, average linkage tends to produce a reasonable number of clusters that are not overly compact or fragmented.

```{r}
d <- dist(pol_scaled0)  # Euclidean distance by default

# Average linkage
hc_result <- hclust(d, method = "average")

# Plot the dendrogram
plot(hc_result)
rect.hclust(hc_result, k = 5)  # Highlight clusters; adjust 'k' as needed

d1 <- dist(pc1_data)  # Euclidean distance by default

# Average linkage
hc_result1 <- hclust(d1, method = "average")

# Plot the dendrogram
plot(hc_result1)
rect.hclust(hc_result1, k = 5)  # Highlight clusters; adjust 'k' as needed

```

See that $k=3$ has the lowest instability, so we stick to that, although we display both of them for safety.

```{r}
set.seed(777)

nclust0 <- nselectboot(data = pol_scaled0, B=100, clustermethod = hclustCBI, 
            method = "average", krange = 4:10)

nclust0$kopt
nclust0$stabk #get the instability values

cl3 <- clusterboot(data = pol_scaled0, B=200, bootmethod = c("boot"),
                      clustermethod = hclustCBI, method = "average", k = 3, seed=777)
cl4 <- clusterboot(data = pol_scaled0, B=200, bootmethod = c("boot"),
                      clustermethod = hclustCBI, method = "average", k = 4, seed=777)
cl5 <- clusterboot(data = pol_scaled0, B=200, bootmethod = c("boot"),
                      clustermethod = hclustCBI, method = "average", k = 5, seed=777)
cl6 <- clusterboot(data = pol_scaled0, B=200, bootmethod = c("boot"),
                      clustermethod = hclustCBI, method = "average", k = 6, seed=777)

# 5 was the largest for which it didn't give an error...

print(cl3)
print(cl4)
print(cl5)
print(cl6)

cl6$result$partition

```

#### Profile the clusters you find and decide for each cluster where on the political spectrum you would place it. Do the cluster profiles agree with expected political spectrum patterns?
